{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200 records.\n"
     ]
    }
   ],
   "source": [
    "#加载数据集\n",
    "import json\n",
    "\n",
    "data = []\n",
    "with open('../data/crag_data_200.jsonl', 'r') as file:\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(data)} records.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前工作目录是: /home/bangx/RAG_2406/RAG_HBX\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 获取并打印当前工作目录\n",
    "current_working_directory = os.getcwd()\n",
    "print(f\"当前工作目录是: {current_working_directory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "local_model_path = \"./model-qxk/checkpoint-15000\"\n",
    "\n",
    "class MyEmbeddingFunction(EmbeddingFunction):\n",
    "    def __init__(self, model_path: str):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.model = AutoModel.from_pretrained(model_path)\n",
    "\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        inputs = self.tokenizer(input, return_tensors='pt', padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        # Assuming the embeddings are in the last hidden state and taking the mean pooling\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "        return embeddings.tolist()\n",
    "\n",
    "\n",
    "# 使用你的bge-small-en-v1.5模型创建自定义嵌入函数\n",
    "my_embedding_function = MyEmbeddingFunction(model_path=local_model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建数据库\n",
    "import chromadb\n",
    "\n",
    "# Initialize Chroma client\n",
    "# client = chromadb.Client()\n",
    "client = chromadb.PersistentClient(path=\"./chroma_pipeline_qxk\")\n",
    "# Create a new collection\n",
    "collection = client.get_collection(name=\"collection_embedding\", embedding_function=my_embedding_function)\n",
    "# collection = client.create_collection('crag_documents')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=64, chunk_overlap=2, add_start_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing records: 100%|██████████| 200/200 [00:01<00:00, 118.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been inserted into Chroma.\n"
     ]
    }
   ],
   "source": [
    "# 准备并插入数据到Chroma\n",
    "from tqdm import tqdm\n",
    "\n",
    "documents = []\n",
    "metadatas = []\n",
    "ids = []\n",
    "\n",
    "for i, record in enumerate(tqdm(data, desc=\"Processing records\")):\n",
    "    query = record[\"query\"]\n",
    "    answer = record[\"answer\"]\n",
    "    search_results = record[\"search_results\"]\n",
    "    \n",
    "    # 创建一个包含query和answer的文档字符串，并添加search_results的内容\n",
    "    # document = query + \" \" + answer + \" \" + \" \".join([result[\"page_snippet\"] for result in search_results])\n",
    "    text = ''.join([result[\"page_snippet\"] for result in search_results])\n",
    "    doc=text_splitter.create_documents([text])\n",
    "    all_splits = text_splitter.split_documents(doc)\n",
    "    \n",
    "    # 遍历所有分块并分别添加到相应的列表中\n",
    "    for j, split in enumerate(all_splits):\n",
    "        documents.append(split.page_content)\n",
    "        metadatas.append({\n",
    "            \"query\": query,\n",
    "            \"answer\": answer,\n",
    "            \"split_index\": j\n",
    "        })\n",
    "        ids.append(f\"doc_{i}_split_{j}\")\n",
    "\n",
    "# 向集合中添加文档\n",
    "collection.add(\n",
    "    documents=documents,  # 自动处理分词、嵌入和索引\n",
    "    metadatas=metadatas,  # 可以根据这些元数据进行过滤\n",
    "    ids=ids  # 每个文档的唯一标识\n",
    ")\n",
    "\n",
    "print(\"Data has been inserted into Chroma.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 relevant documents:\n",
      "Is gold or silver the best choice for investing? We compare the\n",
      "as an investment, silver is not as popular as gold, it is in\n",
      "considering silver as an investment option then you would be\n",
      "considering silver as an investment option then you would be\n",
      "investment option.Though, as an investment, silver is not as\n",
      "choosing investments can be difficult. Some want Gold or Silver\n",
      "investing in silver can be a wise choice. You can also avail\n",
      "silver. To put it into perspective, if you invested £50,000 in\n",
      "for this precious metal. Hence, investing in silver can be a\n",
      "between silver and gold, if you bought both in equal monetary\n",
      "risk. ... Silver can be considered a good portfolio diversifier\n",
      "returns. However, if you are considering silver as an\n",
      "silver. Silver prices have higher price volatility and can be\n",
      "you invested £50,000 in silver, you would need approximately\n",
      "investments can be difficult. Some want Gold or Silver coins,\n",
      "Context:\n",
      "Is gold or silver the best choice for investing? We compare the\n",
      "as an investment, silver is not as popular as gold, it is in\n",
      "considering silver as an investment option then you would be\n",
      "considering silver as an investment option then you would be\n",
      "investment option.Though, as an investment, silver is not as\n",
      "choosing investments can be difficult. Some want Gold or Silver\n",
      "investing in silver can be a wise choice. You can also avail\n",
      "silver. To put it into perspective, if you invested £50,000 in\n",
      "for this precious metal. Hence, investing in silver can be a\n",
      "between silver and gold, if you bought both in equal monetary\n",
      "risk. ... Silver can be considered a good portfolio diversifier\n",
      "returns. However, if you are considering silver as an\n",
      "silver. Silver prices have higher price volatility and can be\n",
      "you invested £50,000 in silver, you would need approximately\n",
      "investments can be difficult. Some want Gold or Silver coins,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 测试用例\n",
    "\n",
    "# 查询\n",
    "query = \"which is a better investment, gold or silver, when considering long-term return?\"\n",
    "\n",
    "# 执行查询，先筛选出包含query的所有文本块，再获取最相似的三个结果\n",
    "results = collection.query(\n",
    "    query_texts=[query],\n",
    "    n_results=15,\n",
    "    where={\"query\": query},  # 筛选包含该query的所有文本块\n",
    ")\n",
    "\n",
    "print(\"Top 3 relevant documents:\")\n",
    "context = \"\"\n",
    "for result in results['documents']:\n",
    "    for doc in result:  # 遍历列表中的每一个文档\n",
    "        print(doc)\n",
    "        context += doc + \"\\n\"\n",
    "\n",
    "print(\"Context:\")\n",
    "print(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from OpenAI: Task decomposition for Large Language Models (LLMs) involves breaking down a complex task into smaller, more manageable sub-tasks that the model can process sequentially or in parallel. This approach allows the LLM to address tasks that require multiple steps or components by dividing them into simpler tasks that the model can handle more effectively.\n",
      "\n",
      "By decomposing tasks, LLM agents can achieve better performance, reduce computational complexity, and improve efficiency in handling a wide range of tasks. Task decomposition can also help LLMs to\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# 设置API密钥\n",
    "os.environ['OPENAI_API_KEY'] = \n",
    "\n",
    "# 设置TOKENIZERS_PARALLELISM环境变量\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# 实例化OpenAI客户端\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "def query_openai(prompt, model=\"gpt-3.5-turbo\", max_tokens=100):\n",
    "    \"\"\"\n",
    "    调用OpenAI API进行查询并获取响应。\n",
    "\n",
    "    :param prompt: 输入的提示文本\n",
    "    :param model: 使用的模型，默认为gpt-3.5-turbo\n",
    "    :param max_tokens: 响应的最大token数，默认为100\n",
    "    :return: OpenAI模型的响应文本\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# 示例查询\n",
    "prompt = \"What is task decomposition for LLM agents?\"\n",
    "try:\n",
    "    response = query_openai(prompt)\n",
    "    print(\"Response from OpenAI:\", response)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_openai_api(query, context):\n",
    "    prompt=f\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Answer in 2 to 3 sentences. \\nQuery: {query}\\nContext: {context}\\n\"\n",
    "\n",
    "    try:\n",
    "        response = query_openai(prompt)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# 设置百度API密钥\n",
    "\n",
    "\n",
    "def get_access_token():\n",
    "    \"\"\"\n",
    "    使用 API Key，Secret Key 获取access_token\n",
    "    \"\"\"\n",
    "    url = f\"https://aip.baidubce.com/oauth/2.0/token?grant_type=client_credentials&client_id={API_KEY}&client_secret={SECRET_KEY}\"\n",
    "    response = requests.post(url)\n",
    "    return response.json().get(\"access_token\")\n",
    "\n",
    "def call_baidu_chat_api(query, context):\n",
    "    access_token = get_access_token()\n",
    "    url = f\"https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/chat/yi_34b_chat?access_token={access_token}\"\n",
    "    payload = json.dumps({\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Answer in 2 to 3 sentences. Context: {context} Question: {query}\",\n",
    "            }\n",
    "        ]\n",
    "    })\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, data=payload)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from Baidu API:\n",
      "The first president of the United States was George Washington. He served two terms from 1789 to 1797 and is often referred to as the \"Father of His Country\" for his significant contributions to the founding of the nation.\n"
     ]
    }
   ],
   "source": [
    "# 测试用例\n",
    "query = \"Who was the first president of the United States?\"\n",
    "context = \"\"\"\n",
    "The first president of the United States was George Washington. He served two terms as president from 1789 to 1797. Washington is often referred to as the \"Father of His Country\" for his pivotal role in the founding of the United States. Before becoming president, he served as the commander-in-chief of the Continental Army during the American Revolutionary War and presided over the convention that drafted the U.S. Constitution.\n",
    "\"\"\"\n",
    "# 调用百度API获取答案\n",
    "response = call_openai_api(query, context)\n",
    "print(\"Response from Baidu API:\")\n",
    "print(response)\n",
    "if \"result\" in response and response[\"result\"]:\n",
    "    print(response[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_database(query):\n",
    "    results = collection.query(\n",
    "    query_texts=[query],\n",
    "    n_results=15,\n",
    "    where={\"query\": query},  # 筛选包含该query的所有文本块\n",
    "                        )\n",
    "    context = \"\"\n",
    "    for result in results['documents']:\n",
    "        for doc in result:  # 遍历列表中的每一个文档\n",
    "            context += doc + \"\\n\"\n",
    "\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_database(query):\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=15,\n",
    "        where={\"query\": query},  # 筛选包含该query的所有文本块\n",
    "    )\n",
    "    \n",
    "    context = \"\"\n",
    "    doc_count = {}  # 记录每个文档的文本块出现次数\n",
    "    doc_texts = {}  # 记录每个文档的文本块内容\n",
    "\n",
    "    for result in results['documents']:\n",
    "        for doc in result:  # 遍历列表中的每一个文档\n",
    "            doc_id = doc.split(\"_split_\")[0]  # 提取doc_{i}部分\n",
    "            split_index = int(doc.split(\"_split_\")[1])  # 提取块索引\n",
    "            if doc_id not in doc_count:\n",
    "                doc_count[doc_id] = 0\n",
    "                doc_texts[doc_id] = {}\n",
    "            doc_count[doc_id] += 1\n",
    "            doc_texts[doc_id][split_index] = doc\n",
    "    \n",
    "    # 阈值定义\n",
    "    threshold = 3\n",
    "    \n",
    "    for doc_id, count in doc_count.items():\n",
    "        if count >= threshold:\n",
    "            # 找到最小和最大的split_index\n",
    "            min_index = min(doc_texts[doc_id].keys())\n",
    "            max_index = max(doc_texts[doc_id].keys())\n",
    "            # 合并并补全同一文档的所有文本块\n",
    "            merged_text = \"\"\n",
    "            for i in range(min_index, max_index + 1):\n",
    "                if i in doc_texts[doc_id]:\n",
    "                    merged_text += doc_texts[doc_id][i] + \"\\n\"\n",
    "                else:\n",
    "                    # 补全缺失的块\n",
    "                    merged_text += f\"{doc_id}_split_{i}\\n\"\n",
    "            context += merged_text + \"\\n\"\n",
    "        else:\n",
    "            # 直接添加到context中\n",
    "            for index in sorted(doc_texts[doc_id].keys()):\n",
    "                context += doc_texts[doc_id][index] + \"\\n\"\n",
    "    \n",
    "    return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_database(query):\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=15,\n",
    "        where={\"query\": query},  # 筛选包含该query的所有文本块\n",
    "    )\n",
    "    \n",
    "    context = \"\"\n",
    "    doc_count = {}  # 记录每个文档的文本块出现次数\n",
    "    doc_texts = {}  # 记录每个文档的文本块内容\n",
    "\n",
    "    for result in results['documents']:\n",
    "        for doc in result:  # 遍历列表中的每一个文档\n",
    "            parts = doc.split(\"_split_\")\n",
    "            if len(parts) == 2:\n",
    "                doc_id = parts[0]  # 提取doc_{i}部分\n",
    "                try:\n",
    "                    split_index = int(parts[1])  # 提取块索引\n",
    "                except ValueError:\n",
    "                    continue  # 忽略无法转换为整数的索引\n",
    "\n",
    "                if doc_id not in doc_count:\n",
    "                    doc_count[doc_id] = 0\n",
    "                    doc_texts[doc_id] = {}\n",
    "                doc_count[doc_id] += 1\n",
    "                doc_texts[doc_id][split_index] = doc\n",
    "            else:\n",
    "                continue  # 忽略不符合预期格式的文档\n",
    "    \n",
    "    # 阈值定义\n",
    "    threshold = 3\n",
    "    \n",
    "    for doc_id, count in doc_count.items():\n",
    "        if count >= threshold:\n",
    "            # 找到最小和最大的split_index\n",
    "            min_index = min(doc_texts[doc_id].keys())\n",
    "            max_index = max(doc_texts[doc_id].keys())\n",
    "            # 合并并补全同一文档的所有文本块\n",
    "            merged_text = \"\"\n",
    "            for i in range(min_index, max_index + 1):\n",
    "                if i in doc_texts[doc_id]:\n",
    "                    merged_text += doc_texts[doc_id][i] + \"\\n\"\n",
    "                else:\n",
    "                    # 补全缺失的块\n",
    "                    merged_text += f\"{doc_id}_split_{i}\\n\"\n",
    "            context += merged_text + \"\\n\"\n",
    "        else:\n",
    "            # 直接添加到context中\n",
    "            for index in sorted(doc_texts[doc_id].keys()):\n",
    "                context += doc_texts[doc_id][index] + \"\\n\"\n",
    "    \n",
    "    return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing records:   0%|          | 1/200 [00:02<09:33,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: which is a better investment, gold or silver, when considering long-term return?\n",
      "Answer: gold\n",
      "Response from Baidu API: When considering long-term return, gold is generally considered a better investment compared to silver. Gold is more popular and often seen as a safer asset with more stable value over time. Silver, on the other hand, tends to have higher price volatility and may not offer the same level of long-term return as gold.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing records:   1%|          | 2/200 [00:03<05:56,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: which country is the largest gold producer?\n",
      "Answer: china\n",
      "Response from Baidu API: China is the largest gold producer in the world, producing 403 tonnes of gold alone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing records:   1%|          | 2/200 [00:05<08:58,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: what is the name of priyanka chopra's fashion line?\n",
      "Answer: invalid question\n",
      "Response from Baidu API: I'm sorry, but based on the provided context, there is no specific mention of the name of Priyanka Chopra's fashion line.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#测试用例，先问它个5回\n",
    "from tqdm import tqdm\n",
    "\n",
    "for i, record in enumerate(tqdm(data, desc=\"Processing records\")):\n",
    "    query = record[\"query\"]\n",
    "    answer = record[\"answer\"]\n",
    "    search_results = record[\"search_results\"]\n",
    "    context = query_database(query)\n",
    "    response = call_openai_api(query, context)\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "    # pred = response['result']\n",
    "    print(f\"Response from Baidu API: {response}\")\n",
    "    if i > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for i, record in enumerate(tqdm(data, desc=\"Processing records\")):\n",
    "    query = record[\"query\"]\n",
    "    answer = record[\"answer\"]\n",
    "    search_results = record[\"search_results\"]\n",
    "    context = query_database(query)\n",
    "    response = call_baidu_chat_api(query, context)\n",
    "    pred = response['result']\n",
    "    result.append(json.dumps({'query': query, 'answer': answer, 'pred': pred}, ensure_ascii=False) + '\\n')\n",
    "with open(\"../data/baidu_chat_results.jsonl\", 'w', encoding='utf-8') as f:\n",
    "    f.write(''.join(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_processing(data, target_file='./baidu_chat_results.jsonl'):\n",
    "    # 检查目标文件的目录是否存在，不存在则创建\n",
    "    target_dir = os.path.dirname(target_file)\n",
    "    if not os.path.exists(target_dir):\n",
    "        os.makedirs(target_dir)\n",
    "\n",
    "    # 读取已有文件的行数，确定从哪一行开始继续处理\n",
    "    if os.path.exists(target_file):\n",
    "        with open(target_file, 'r', encoding='utf-8') as f:\n",
    "            processed_lines = len(f.readlines())\n",
    "    else:\n",
    "        processed_lines = 0\n",
    "\n",
    "    result = []\n",
    "    for i, record in enumerate(tqdm(data[processed_lines:], desc=\"Processing records\", initial=processed_lines, total=len(data))):\n",
    "        query = record[\"query\"]\n",
    "        answer = record[\"answer\"]\n",
    "        search_results = record[\"search_results\"]\n",
    "        context = query_database(query)\n",
    "        response = call_openai_api(query, context)\n",
    "        if response:\n",
    "            pred = response\n",
    "        else:\n",
    "            print(f\"Missing 'result' key in response for query: {query}\")\n",
    "            pred = None\n",
    "        result.append(json.dumps({'query': query, 'answer': answer, 'pred': pred}, ensure_ascii=False) + '\\n')\n",
    "\n",
    "        # 每处理一条记录就立即写入文件，防止中途出错数据丢失\n",
    "        with open(target_file, 'a', encoding='utf-8') as f:\n",
    "            f.write(result[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing records: 100%|██████████| 200/200 [04:50<00:00,  1.45s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "continue_processing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_processing(data, target_file='./baidu_chat_results.jsonl'):\n",
    "    # 检查目标文件的目录是否存在，不存在则创建\n",
    "    target_dir = os.path.dirname(target_file)\n",
    "    if not os.path.exists(target_dir):\n",
    "        os.makedirs(target_dir)\n",
    "\n",
    "    existing_data = []\n",
    "    missing_results_indices = []\n",
    "\n",
    "    # 读取已有文件的内容，找出pred为null的记录\n",
    "    if os.path.exists(target_file):\n",
    "        with open(target_file, 'r', encoding='utf-8') as f:\n",
    "            for idx, line in enumerate(f):\n",
    "                record = json.loads(line)\n",
    "                existing_data.append(record)\n",
    "                if record['pred'] is None:\n",
    "                    missing_results_indices.append(idx)\n",
    "\n",
    "    # 处理缺失 result 的记录\n",
    "    if missing_results_indices:\n",
    "        print(f\"Reprocessing {len(missing_results_indices)} missing results...\")\n",
    "        for idx in tqdm(missing_results_indices, desc=\"Reprocessing missing records\"):\n",
    "            record = existing_data[idx]\n",
    "            query = record[\"query\"]\n",
    "            context = query_database(query)\n",
    "            response = call_openai_api(query, context)\n",
    "            if response:\n",
    "                record['pred'] = response\n",
    "            else:\n",
    "                print(f\"Still missing 'result' key in response for query: {query}\")\n",
    "                record['pred'] = None\n",
    "\n",
    "    # 将所有处理后的记录按原始顺序写回文件\n",
    "    with open(target_file, 'w', encoding='utf-8') as f:\n",
    "        for record in existing_data:\n",
    "            f.write(json.dumps(record, ensure_ascii=False) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
